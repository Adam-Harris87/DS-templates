{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "067c6c6b",
   "metadata": {},
   "source": [
    "<!-- <div style='background-color: orange'> -->\n",
    "<a id=\"TableOfContents\"></a>\n",
    "    <h1 style='text-align: center'>\n",
    "        <b>\n",
    "            Table of Contents:\n",
    "        </b></h1>\n",
    "<li><a href='#imports'>Imports</a></li>\n",
    "<li><a href=\"#acquire\">Acquire</a></li>\n",
    "<li><a href='#prepare'>Prepare</a></li>\n",
    "<li><a href=\"#wrangle\">Wrangle</a></li>\n",
    "<li><a href='#misc'>Miscellaneous</a></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793b55d6",
   "metadata": {},
   "source": [
    "<!-- <div style='background-color: orange'> -->\n",
    "<a id=\"imports\"></a>\n",
    "    <h1 style='text-align: center'>\n",
    "        <b>\n",
    "            Imports\n",
    "        </b></h1>\n",
    "<li><a href='#TableOfContents'>Table of Contents</a></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beb5f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data manipulation tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import file access tools\n",
    "import os\n",
    "import requests\n",
    "# import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import variable encoding function\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# import scaler function\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# import env file containing username and password for database connection\n",
    "# import env\n",
    "import sample_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cd970a",
   "metadata": {},
   "source": [
    "<!-- <div style='background-color: orange'> -->\n",
    "<a id=\"acquire\"></a>\n",
    "    <h1 style='text-align: center'>\n",
    "        <b>\n",
    "                Acquisition\n",
    "        </b></h1>\n",
    "<li><a href='#TableOfContents'>Table of Contents</a></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c730e1",
   "metadata": {},
   "source": [
    "Acquire data via SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0df65b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire_sql_data():\n",
    "    '''\n",
    "    this will read in the dataset from csv file in the local directory,\n",
    "    if one does not exist, it will download the data via sql from the server assigned\n",
    "    in the env file\n",
    "    '''\n",
    "    # set file name\n",
    "    filename = 'sample_file.csv'\n",
    "    # check if file exists in local directory\n",
    "    if os.path.exists(filename):\n",
    "        print('opening file from local directory')\n",
    "        # read in the file to a dataframe\n",
    "        df = pd.read_csv(filename, \n",
    "                         index_col=0, # set the index to the first column\n",
    "                         \n",
    "                         )\n",
    "    # if the file doesn't exist\n",
    "    else:\n",
    "        # print an error\n",
    "        print(f'file {filename} not found in local directory, downloading via SQL')\n",
    "        # download dataset from codeup server\n",
    "        # set database scheme we are looking at\n",
    "        database = 'db_schema' \n",
    "        # create sql connectioni\n",
    "        connection = env.get_db_url(database)\n",
    "        # create sql query\n",
    "        query = '''\n",
    "select *\n",
    "from schema\n",
    ";\n",
    "        '''\n",
    "        # query server via sql\n",
    "        df = pd.read_sql(query, connection)\n",
    "        # cache the data to local csv file\n",
    "        df.to_csv(filename)\n",
    "    # return the dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db9f540",
   "metadata": {},
   "source": [
    "Acquire data from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecbc390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url, offset=1000, max_pages=200):\n",
    "    '''\n",
    "    this function will return a dataFrame with results from the passed api url\n",
    "    and loop through the data until the last page of results, or after 200 pages\n",
    "    '''\n",
    "    print('downloading data via api')\n",
    "    # create request.get for passed url\n",
    "    page = requests.get(url)\n",
    "    # create a dataframe withe the first page results\n",
    "    df = pd.DataFrame(page.json())\n",
    "    # set first offset amount\n",
    "    x=offset\n",
    "    #\n",
    "    print\n",
    "    # loop through all pages of results for passed url\n",
    "    while page.json():\n",
    "        # set the page url to the next page url\n",
    "        next_url = url + f'&$offset={x}'\n",
    "        page = requests.get(next_url)\n",
    "        # add current page data to the dataframe\n",
    "        df = pd.concat([df, pd.DataFrame(page.json())], axis=0)\n",
    "        x += offset\n",
    "        # display current number of rows collected\n",
    "        print(f'{x}')\n",
    "        # break the loop if max number of pages is reached\n",
    "        if x >= offset * max_pages:\n",
    "            break\n",
    "    # display how many rows were downloaded\n",
    "    print(f'{df.shape[0]} records were downloaded via api')\n",
    "    # return the datframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2271429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire_api_data():\n",
    "    '''\n",
    "    This function will read in datasets df1 and df2 data via csv,\n",
    "    if the datasets don't exist in the local directory it will \n",
    "    it will then cache the data into csv files and return the dfs.\n",
    "    '''\n",
    "    # set the filenames we are looking for in the local directory\n",
    "    df1_filename = 'sample_df1.csv'\n",
    "    df2_filename = 'sample_df2.csv'\n",
    "    # set the api paths\n",
    "    token = 'api_token'\n",
    "    df1_api = f'https://data.youWant.com/api/api1Path.json?$$app_token={token}'\n",
    "    df2_api = f'https://data.youWant.com/api/api2Path.json?$$app_token={token}'\n",
    "    \n",
    "    # check if df1 files exist in the local directory\n",
    "    if os.path.exists(df1_filename):\n",
    "        # read in csv files for df1 dataset\n",
    "        print('reading df1 dataset from local file')\n",
    "        df1 = pd.read_csv(df1_filename, index_col=0)\n",
    "    else:\n",
    "        # if the df1 file does not exist locally, then download the data via api\n",
    "        print('df1 data not found in local directory')\n",
    "        df1 = get_data(df1_api)\n",
    "        df1.to_csv(df1_filename, index=False)\n",
    "        \n",
    "    # check if df2 file exist in the local directory\n",
    "    if os.path.exists(df2_filename):\n",
    "        # read in csv files for df2 for second dataset\n",
    "        print('reading df2 data from local file')\n",
    "        df2 = pd.read_csv(df2_filename, index_col=0)\n",
    "    else:\n",
    "        # if the df2 file does not exist locally, then download the data via api\n",
    "        print('df2 data not found in local directory')\n",
    "        df2 = get_data(df2_api)\n",
    "        df2.to_csv(df2_filename)\n",
    "    \n",
    "    # return the combined dataframe\n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc72510",
   "metadata": {},
   "source": [
    "<!-- <div style='background-color: orange'> -->\n",
    "<a id=\"prepare\"></a>\n",
    "    <h1 style='text-align: center'>\n",
    "        <b>\n",
    "            Prepare\n",
    "        </b></h1>\n",
    "<li><a href='#TableOfContents'>Table of Contents</a></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012d6fc7",
   "metadata": {},
   "source": [
    "Sample data preparation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a6552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df1, df2=None):\n",
    "    '''\n",
    "    this will prepare the dataframe(s) for use\n",
    "    '''\n",
    "    \n",
    "    # merge dataframes into one\n",
    "#     df = pd.merge(left=df1, right=df2, how='inner', \n",
    "#                        on='id', suffixes=('_df1','_df2'))\n",
    "    \n",
    "    # rename columns to lowercase, replace spaces and any . characters with _\n",
    "    df.columns = [col.lower().\\\n",
    "                  replace(' ', '_').\\\n",
    "                  replace('.', '_') for col in df.columns]\n",
    "    \n",
    "    \n",
    "    # Replace white space values with NaN values.\n",
    "#     grades = grades.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    \n",
    "    # change a datetime column to datetime type\n",
    "#     df.datetime = pd.to_datetime(df.datetime)\n",
    "    #set datetime as the index\n",
    "#     df = df.set_index('datetime')\n",
    "\n",
    "    # rename columns\n",
    "#     df = df.rename(columns={'old_column_1':'new_column_1',\n",
    "#                             'old_column_2':'new_column_2',\n",
    "#                             'old_column_3':'new_column_3'})\n",
    "    \n",
    "    # create a new column with binned info\n",
    "#     df['new_col'] = np.where(df.outcome_type == 'new_bin_value_1', 'old_value_1',\n",
    "#                        np.where(df.outcome_type == 'new_bin_value_1', 'old_value_2',\n",
    "#                        np.where(df.outcome_type == 'new_bin_value_2', 'old_value_3',\n",
    "#                                )))\n",
    "\n",
    "    # encode categorical variables\n",
    "    # make a list of columns to encode\n",
    "#     encode_cols = ['intake_type', 'intake_condition', 'animal_type', 'sex_upon_intake',\n",
    "#                'breed', 'color', 'sex_upon_outcome', 'outcome_subtype']\n",
    "    \n",
    "#     encode_cat_variables(df, encode_cols)\n",
    "#     one_hot_encode_column(df, cols)\n",
    "    \n",
    "    # split one column into multiple columns\n",
    "#     new_vars = df.col_to_split.str.split(pat=',', expand=True)\n",
    "    # rename the new columns\n",
    "#     new_vars.columns = ['new_col_1', 'new_col_2', 'new_col_3']\n",
    "    # Concatenate new columns to the original dataframe\n",
    "#     df = pd.concat([df, new_vars], axis=1)\n",
    "\n",
    "#     # return the prepared dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d17372a",
   "metadata": {},
   "source": [
    "Encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3ae590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cat_variables(df, encode_cols):\n",
    "    # encode categorical columns into numerical values that can be used in modeling\n",
    "    # create encoder object\n",
    "    le = LabelEncoder()\n",
    "    for col in encode_cols:\n",
    "        le.fit(df[col])\n",
    "        # create a new column with the encoded values\n",
    "        df[f'{col}_encoded'] = le.transform(df[col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa279d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_columns(df, cols):\n",
    "    # one-hot encode the outcome variable since that is our target\n",
    "    df = pd.concat([df, \n",
    "                    (df[[cols]], \n",
    "                     dummy_na=False, \n",
    "                     drop_first=[True, True])], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfd8095",
   "metadata": {},
   "source": [
    "Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77ff27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df):\n",
    "    '''\n",
    "    This function splits a dataframe into \n",
    "    train, validate, and test in order to explore the data and to create and validate models. \n",
    "    It takes in a dataframe and contains an integer for setting a seed for replication. \n",
    "    Test is 20% of the original dataset. The remaining 80% of the dataset is \n",
    "    divided between valiidate and train, with validate being .30*.80= 24% of \n",
    "    the original dataset, and train being .70*.80= 56% of the original dataset. \n",
    "    The function returns, train, validate and test dataframes. \n",
    "    '''\n",
    "    # Here we are spliting train into .8 of the original dataset. \n",
    "    # and test into 20% of the original dataset.\n",
    "    train, test = train_test_split(df, test_size = .2, random_state=123)\n",
    "    # here we assign validate to be .3 of the train dataset \n",
    "    train, validate = train_test_split(train, test_size=.3, random_state=123)\n",
    "    # returns train validate and test dataframes\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c707c4",
   "metadata": {},
   "source": [
    "<!-- <div style='background-color: orange'> -->\n",
    "<a id=\"wrangle\"></a>\n",
    "    <h1 style='text-align: center'>\n",
    "        <b>\n",
    "            Wrangle\n",
    "        </b></h1>\n",
    "<li><a href='#TableOfContents'>Table of Contents</a></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de9bf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_data():\n",
    "    '''\n",
    "    this will perform both acquire and preparation steps in one command,\n",
    "    then will split the data into train, validate and test\n",
    "    '''\n",
    "    # acquire the dataset\n",
    "    df1 = acquire_sql_data()\n",
    "    # prepare the data\n",
    "    df = prepare_data(df1)\n",
    "    # split data into train, validate and test groups\n",
    "    train, validate, test = split_data(df)\n",
    "    # return the original and split dfs\n",
    "    return df, train, validate, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3853ba",
   "metadata": {},
   "source": [
    "<!-- <div style='background-color: orange'> -->\n",
    "<a id=\"misc\"></a>\n",
    "    <h1 style='text-align: center'>\n",
    "        <b>\n",
    "            Miscellaneous\n",
    "        </b></h1>\n",
    "<li><a href='#TableOfContents'>Table of Contents</a></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cc1dd3",
   "metadata": {},
   "source": [
    "Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fdbd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(train, \n",
    "               validate, \n",
    "               test, \n",
    "               columns_to_scale=scale_cols,\n",
    "               scaler=MinMaxScaler(),\n",
    "               return_scaler=False):\n",
    "    '''\n",
    "    Scales the 3 data splits. \n",
    "    Takes in train, validate, and test data splits and returns their scaled counterparts.\n",
    "    If return_scalar is True, the scaler object will be returned as well\n",
    "    '''\n",
    "    # make copies of our original data so we dont gronk up anything\n",
    "    train_scaled = train.copy()\n",
    "    validate_scaled = validate.copy()\n",
    "    test_scaled = test.copy()\n",
    "    \n",
    "    #     fit the thing\n",
    "    scaler.fit(train[columns_to_scale])\n",
    "    # applying the scaler:\n",
    "    train_scaled[columns_to_scale] = pd.DataFrame(\n",
    "        scaler.transform(train[columns_to_scale]),\n",
    "        columns=train[columns_to_scale].columns.values, \n",
    "        index = train.index)\n",
    "    # apply the scaller on the validation dataset                                    \n",
    "    validate_scaled[columns_to_scale] = pd.DataFrame(\n",
    "        scaler.transform(validate[columns_to_scale]),\n",
    "        columns=validate[columns_to_scale].columns.values).set_index(\n",
    "        [validate.index.values])\n",
    "    # apply the scaler on the test dataset\n",
    "    test_scaled[columns_to_scale] = pd.DataFrame(scaler.transform(\n",
    "        test[columns_to_scale]), \n",
    "        columns=test[columns_to_scale].columns.values).set_index(\n",
    "        [test.index.values])\n",
    "    \n",
    "    # if we requested the scaler returned in the function call,\n",
    "    # then return the scaler object along with the scaled data\n",
    "    if return_scaler:\n",
    "        return scaler, train_scaled, validate_scaled, test_scaled\n",
    "    # otherwise return the scaled data\n",
    "    else:\n",
    "        return train_scaled, validate_scaled, test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e18c03d",
   "metadata": {},
   "source": [
    "Impute null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6ae3b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f9c4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
