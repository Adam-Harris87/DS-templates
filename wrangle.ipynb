{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "067c6c6b",
   "metadata": {},
   "source": [
    "<!-- <div style='background-color: orange'> -->\n",
    "<a id=\"TableOfContents\"></a>\n",
    "    <h1 style='text-align: center'>\n",
    "        <b>\n",
    "            Table of Contents:\n",
    "        </b></h1>\n",
    "<li><a href='#imports'>Imports</a></li>\n",
    "<li><a href=\"#acquire\">Acquire</a></li>\n",
    "<li><a href='#prepare'>Prepare</a></li>\n",
    "<li><a href=\"#wrangle\">Wrangle</a></li>\n",
    "<li><a href='#misc'>Miscellaneous</a></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793b55d6",
   "metadata": {},
   "source": [
    "<!-- <div style='background-color: orange'> -->\n",
    "<a id=\"imports\"></a>\n",
    "    <h1 style='text-align: center'>\n",
    "        <b>\n",
    "            Imports\n",
    "        </b></h1>\n",
    "<li><a href='#TableOfContents'>Table of Contents</a></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beb5f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data manipulation tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import file access tools\n",
    "import os\n",
    "import requests\n",
    "# import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import variable encoding function\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# import env file containing username and password for database connection\n",
    "# import env\n",
    "import sample_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cd970a",
   "metadata": {},
   "source": [
    "<!-- <div style='background-color: orange'> -->\n",
    "<a id=\"acquire\"></a>\n",
    "    <h1 style='text-align: center'>\n",
    "        <b>\n",
    "                Acquisition\n",
    "        </b></h1>\n",
    "<li><a href='#TableOfContents'>Table of Contents</a></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cef1905",
   "metadata": {},
   "source": [
    "Acquire data via SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0df65b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire_sql_data():\n",
    "    '''\n",
    "    this will read in the dataset from csv file in the local directory,\n",
    "    if one does not exist, it will download the data via sql from the server assigned\n",
    "    in the env file\n",
    "    '''\n",
    "    # set file name\n",
    "    filename = 'sample_file.csv'\n",
    "    # check if file exists in local directory\n",
    "    if os.path.exists(filename):\n",
    "        print('opening file from local directory')\n",
    "        # read in the file to a dataframe\n",
    "        df = pd.read_csv(filename, \n",
    "                         index_col=0, # set the index to the first column\n",
    "                         \n",
    "                         )\n",
    "    # if the file doesn't exist\n",
    "    else:\n",
    "        # print an error\n",
    "        print(f'file {filename} not found in local directory, downloading via SQL')\n",
    "        # download dataset from codeup server\n",
    "        # set database scheme we are looking at\n",
    "        database = 'db_schema' \n",
    "        # create sql connectioni\n",
    "        connection = env.get_db_url(database)\n",
    "        # create sql query\n",
    "        query = '''\n",
    "select *\n",
    "from schema\n",
    ";\n",
    "        '''\n",
    "        # query server via sql\n",
    "        df = pd.read_sql(query, connection)\n",
    "        # cache the data to local csv file\n",
    "        df.to_csv(filename)\n",
    "    # return the dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2306956",
   "metadata": {},
   "source": [
    "Acquire data from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b0b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url, offset=1000, max_pages=200):\n",
    "    '''\n",
    "    this function will return a dataFrame with results from the passed api url\n",
    "    and loop through the data until the last page of results, or after 200 pages\n",
    "    '''\n",
    "    print('downloading data via api')\n",
    "    # create request.get for passed url\n",
    "    page = requests.get(url)\n",
    "    # create a dataframe withe the first page results\n",
    "    df = pd.DataFrame(page.json())\n",
    "    # set first offset amount\n",
    "    x=offset\n",
    "    #\n",
    "    print\n",
    "    # loop through all pages of results for passed url\n",
    "    while page.json():\n",
    "        # set the page url to the next page url\n",
    "        next_url = url + f'&$offset={x}'\n",
    "        page = requests.get(next_url)\n",
    "        # add current page data to the dataframe\n",
    "        df = pd.concat([df, pd.DataFrame(page.json())], axis=0)\n",
    "        x += offset\n",
    "        # display current number of rows collected\n",
    "        print(f'{x}')\n",
    "        # break the loop if max number of pages is reached\n",
    "        if x >= offset * max_pages:\n",
    "            break\n",
    "    # display how many rows were downloaded\n",
    "    print(f'{df.shape[0]} records were downloaded via api')\n",
    "    # return the datframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36044ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire_api_data():\n",
    "    '''\n",
    "    This function will read in datasets df1 and df2 data via csv,\n",
    "    if the datasets don't exist in the local directory it will \n",
    "    it will then cache the data into csv files and return the dfs.\n",
    "    '''\n",
    "    # set the filenames we are looking for in the local directory\n",
    "    df1_filename = 'sample_df1.csv'\n",
    "    df2_filename = 'sample_df2.csv'\n",
    "    # set the api paths\n",
    "    token = 'api_token'\n",
    "    df1_api = f'https://data.youWant.com/api/api1Path.json?$$app_token={token}'\n",
    "    df2_api = f'https://data.youWant.com/api/api2Path.json?$$app_token={token}'\n",
    "    \n",
    "    # check if df1 files exist in the local directory\n",
    "    if os.path.exists(df1_filename):\n",
    "        # read in csv files for df1 dataset\n",
    "        print('reading df1 dataset from local file')\n",
    "        df1 = pd.read_csv(df1_filename, index_col=0)\n",
    "    else:\n",
    "        # if the df1 file does not exist locally, then download the data via api\n",
    "        print('df1 data not found in local directory')\n",
    "        df1 = get_data(df1_api)\n",
    "        df1.to_csv(df1_filename, index=False)\n",
    "        \n",
    "    # check if df2 file exist in the local directory\n",
    "    if os.path.exists(df2_filename):\n",
    "        # read in csv files for df2 for second dataset\n",
    "        print('reading df2 data from local file')\n",
    "        df2 = pd.read_csv(df2_filename, index_col=0)\n",
    "    else:\n",
    "        # if the df2 file does not exist locally, then download the data via api\n",
    "        print('df2 data not found in local directory')\n",
    "        df2 = get_data(df2_api)\n",
    "        df2.to_csv(df2_filename)\n",
    "    \n",
    "    # return the combined dataframe\n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc72510",
   "metadata": {},
   "source": [
    "<!-- <div style='background-color: orange'> -->\n",
    "<a id=\"prepare\"></a>\n",
    "    <h1 style='text-align: center'>\n",
    "        <b>\n",
    "            Prepare\n",
    "        </b></h1>\n",
    "<li><a href='#TableOfContents'>Table of Contents</a></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa6c57f",
   "metadata": {},
   "source": [
    "Sample data preparation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424afd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df1, df2=None):\n",
    "    '''\n",
    "    this will prepare the dataframe(s) for use\n",
    "    '''\n",
    "    \n",
    "    # merge dataframes into one\n",
    "#     df = pd.merge(left=df1, right=df2, how='inner', \n",
    "#                        on='id', suffixes=('_df1','_df2'))\n",
    "    \n",
    "    # rename columns to lowercase, replace spaces and any . characters with _\n",
    "    df.columns = [col.lower().\\\n",
    "                  replace(' ', '_').\\\n",
    "                  replace('.', '_') for col in df.columns]\n",
    "    \n",
    "    # change a datetime column to datetime type\n",
    "#     df.datetime = pd.to_datetime(df.datetime)\n",
    "    #set datetime as the index\n",
    "#     df = df.set_index('datetime')\n",
    "\n",
    "    # rename columns\n",
    "#     df = df.rename(columns={'old_column_1':'new_column_1',\n",
    "#                             'old_column_2':'new_column_2',\n",
    "#                             'old_column_3':'new_column_3'})\n",
    "    \n",
    "    # create a new column with binned info\n",
    "#     df['new_col'] = np.where(df.outcome_type == 'new_bin_value_1', 'old_value_1',\n",
    "#                        np.where(df.outcome_type == 'new_bin_value_1', 'old_value_2',\n",
    "#                        np.where(df.outcome_type == 'new_bin_value_2', 'old_value_3',\n",
    "#                                )))\n",
    "\n",
    "    # encode categorical variables\n",
    "    \n",
    "#     # convert first part of website path into column called lesson\n",
    "#     # create an empty list\n",
    "#     page_list=[]\n",
    "#     # split the webpage path by '/'\n",
    "#     pages = df.path.str.split('/')\n",
    "#     # go through all of the pages\n",
    "#     for i in range(len(pages)):\n",
    "#         # make a list of the first part of the paths\n",
    "#         page_list.append(pages[i][0])\n",
    "#     # create a new column with the first part of the path\n",
    "#     df['lesson'] = page_list\n",
    "\n",
    "#     # return the prepared dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36c2aef",
   "metadata": {},
   "source": [
    "Encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042ee572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cat_variables(df, encode_cols):\n",
    "    # encode categorical columns so we can use them during explore and modeling\n",
    "    # make a list of columns to encode\n",
    "    encode_cols = ['intake_type', 'intake_condition', 'animal_type', 'sex_upon_intake',\n",
    "               'breed', 'color', 'sex_upon_outcome', 'outcome_subtype']\n",
    "    # create encoder object\n",
    "    le = LabelEncoder()\n",
    "    for col in encode_cols:\n",
    "        le.fit(df[col])\n",
    "        # create a new column with the encoded values\n",
    "        df[f'{col}_encoded'] = le.transform(df[col])\n",
    "    # we want to one-hot encode the outcome variable since that is our target\n",
    "    df = pd.concat([df, pd.get_dummies(df.outcome)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c707c4",
   "metadata": {},
   "source": [
    "<!-- <div style='background-color: orange'> -->\n",
    "<a id=\"wrangle\"></a>\n",
    "    <h1 style='text-align: center'>\n",
    "        <b>\n",
    "            Wrangle\n",
    "        </b></h1>\n",
    "<li><a href='#TableOfContents'>Table of Contents</a></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de9bf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_web_traffic():\n",
    "    '''\n",
    "    this will perform both acquire and preparation steps in one command\n",
    "    '''\n",
    "    # acquire and prepare data\n",
    "    df = prepare_data(acquire_sql_data())\n",
    "    # split data into train, validate and test groups\n",
    "    train, validate, test = split_data(df)\n",
    "    # return the original and split dfs\n",
    "    return df, train, validate, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3853ba",
   "metadata": {},
   "source": [
    "<!-- <div style='background-color: orange'> -->\n",
    "<a id=\"misc\"></a>\n",
    "    <h1 style='text-align: center'>\n",
    "        <b>\n",
    "            Miscellaneous\n",
    "        </b></h1>\n",
    "<li><a href='#TableOfContents'>Table of Contents</a></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4368ea77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
